from pylab import *
import random
from scipy import sparse
from scipy.sparse import csc_matrix
import numpy as np
import matplotlib.pyplot as plt
import sklearn
from sklearn import linear_model
from sklearn.preprocessing import normalize
from mpi4py import MPI
comm = MPI.COMM_WORLD
rank = MPI.COMM_WORLD.Get_rank()
logistic = linear_model.LogisticRegression()



import sentence_dictionaries
from sentence_dictionaries import create_sentence


# NUMBER = length of the output
NUMBER = 5000

# network size = number of excitatory neurons
networksize = [150,200,300,400,500,600,700,800,900,1000,1100,1200,1300,1400,1500]
N_E = networksize[rank]

# number of inhibitory and input neurons
N_I = int(0.2 * N_E)
N_U = int(round(0.025 * N_E))

# letterstring = string that encodes the sentence
letterstring = create_sentence(N_E,N_U)[0]

# T_plastic = time steps with plasticity
# T_train = size of the readout

plastic_timesteps = [50000]
T_plastic = plastic_timesteps[0]
T_train = 5000                                         
Timesteps = T_plastic + 2*T_train

# inputsize = length of the input, length of one sentence
inputsize = shape(letterstring)[0]

# Y saves the labels (letters) of all sentences that are shown to the network
Y = zeros((Timesteps))



## INPUT:
#input vector u
u = eye(inputsize)

W_eu_pre = zeros((shape(letterstring)[1],shape(letterstring)[0]))
for i in range(shape(letterstring)[0]):
    W_eu_pre[:,i] = letterstring[i]

W_eu = zeros((N_E,inputsize))
W_eu[0:shape(W_eu_pre)[0],0:shape(W_eu_pre)[1]] = W_eu_pre



# Plasticity values
H_IP = 0.1
eta_STDP = 0.001
eta_IP = 0.001

T_e_max = 0.5
T_i_max = 0.5

Lambda = 10

# T_e und T_i
T_e = rand(N_E) * T_e_max
T_i = rand(N_I) * T_i_max

# x und y
x = rand(N_E, Timesteps + 1 + NUMBER)
x[x >= 0.5] = 1
x[x < 0.5] = 0

y = rand(N_I)
y[y >= 0.5] = 1
y[y < 0.5] = 0

# Matrix bei der die Diagonalelemente 0 sind und alle anderen 1
MatrixDiag = ones((N_E,N_E))
for i in range(N_E):
    for j in range(N_E):
        if i == j:
            MatrixDiag[i,j] = 0


## W_ee:
lam = 1 - float(Lambda)/N_E

cc = 1
while cc > 0:
    cc = 0
    W_ee = rand(N_E, N_E)
    W_ee[W_ee < lam] = 0
    W_ee[W_ee > 0] = rand(sum(W_ee > 0))
    summe = np.sum(W_ee, axis = 1)
    for n in summe:
        if n == 0:
            cc = 1
            break

# alle Diagonalelemente in W_ee auf null setzen
W_ee = W_ee * MatrixDiag

# Normalisierung W_ee
W_ee = normalize(W_ee , norm='l1' ,  axis = 1 , copy=False)

## W_ei und W_ie:
W_ei = rand(N_E, N_I)
W_ie = rand(N_I, N_E)

# Normalisierung W_ei
W_ei = normalize(W_ei , norm='l1' ,  axis = 1 , copy=False)

# Normalisierung W_ie
W_ie = normalize(W_ie , norm='l1' ,  axis = 1 , copy=False)

# Sparse Matrix
W_ee = sparse.csc_matrix(W_ee)
row = W_ee.indices
col = np.repeat(arange(N_E),np.diff(W_ee.indptr))
data = W_ee.data



iter = 0
#SORN
for t in range(Timesteps):

    # Feeding sentence letter by letter into the SORN, after one sentence is completet
    # a new sentence is picked.
    # When iter == inputsize, sentence is completed and a new one is picked

    if iter == inputsize:

        reload(sentence_dictionaries)
        from sentence_dictionaries import create_sentence

        letterstring = create_sentence(N_E,N_U)[0]

        inputsize = shape(letterstring)[0]

        u = eye(inputsize)

        W_eu_pre = zeros((shape(letterstring)[1],shape(letterstring)[0]))
        for i in range(shape(letterstring)[0]):
            W_eu_pre[:,i] = letterstring[i]

        W_eu = zeros((N_E,inputsize))
        W_eu[0:shape(W_eu_pre)[0],0:shape(W_eu_pre)[1]] = W_eu_pre

        iter = 0


    if t >= T_plastic:
        eta_STDP = 0
        Rx = W_ee.dot(x[:,t]) - np.dot(W_ei, y) - T_e + np.dot(W_eu, u[:,iter])

    if t < T_plastic:
        Rx = W_ee.dot(x[:,t]) - np.dot(W_ei, y) - T_e + np.dot(W_eu, u[:,iter]) #+ np.random.normal(0,standarddeviation,N_E)


    Rx[Rx >= 0] = 1
    Rx[Rx < 0] = 0
    x[:,t + 1] = Rx

    # Plasticity rules
    # STDP:
    data += eta_STDP * (x[row,t + 1] * x[col,t] - x[row,t] * x[col,t + 1])
    data[data < 0] = 0

    # IP:
    T_e = T_e + eta_IP * (x[:,t] - H_IP)

    # SN:
    row_sums = np.array(W_ee.sum(1))[:,0]
    data /= row_sums[row]

    Ry = np.dot(W_ie, x[:,t + 1]) - T_i

    Ry[Ry >= 0] = 1
    Ry[Ry < 0] = 0
    y = Ry



    Y[t] = find(W_eu[:,iter]==1)[0]//N_U

    iter = iter + 1



# Predicting letter in sequence, not position

# Training readouts
X_train = x[:,T_plastic:T_plastic + T_train - 1].T
y_train = Y[T_plastic:T_plastic + T_train - 1]

# Test set
X_test = x[:,T_plastic + T_train:T_plastic + 2*T_train].T
y_test = Y[T_plastic + T_train:T_plastic + 2*T_train]

# logistic regression
B = logistic.fit(X_train, y_train).predict(X_test)
Seq = B




######################################
# Put predicted output as next input #
######################################

FIT = logistic.fit(X_train, y_train)

Seq = zeros((NUMBER+1))
Seq[0] = B[0]

#SORN
for t in range(NUMBER):

    eta_STDP = 0

    ## INPUT:
    #input vector u
    u = zeros((inputsize,N_E))
    u[0,0] = 1

    W_eu[:, 0] = 0
    W_eu[int(Seq[t])*N_U:(int(Seq[t])+1)*N_U,0] =  1


    Rx = W_ee.dot(x[:,Timesteps + t]) - np.dot(W_ei, y) - T_e + np.dot(W_eu, u[:,0])

    Rx[Rx >= 0] = 1
    Rx[Rx < 0] = 0
    x[:,Timesteps + t + 1] = Rx

    # STDP:
    data += eta_STDP * (x[row,Timesteps + t + 1] * x[col,Timesteps + t] - x[row,Timesteps + t] * x[col,Timesteps + t + 1])
    data[data < 0] = 0

    # IP:
    T_e = T_e + eta_IP * (x[:,Timesteps + t] - H_IP)

    # SN:
    row_sums = np.array(W_ee.sum(1))[:,0]
    data /= row_sums[row]

    Ry = np.dot(W_ie, x[:,Timesteps + t + 1]) - T_i

    Ry[Ry >= 0] = 1
    Ry[Ry < 0] = 0
    y = Ry




    Y_pre = find(W_eu[:,0]==1)[0]//N_U

    Y = append(Y,Y_pre)


    # Prediction
    #X_train = x[:,T_plastic:T_plastic + T_train  + t].T
    #y_train = Y[T_plastic:T_plastic + T_train + t]

    X_train = x[:,T_plastic + T_train:T_plastic + 2*T_train].T
    y_train = Y[T_plastic + T_train:T_plastic + 2*T_train]



    X_test = x[:,T_plastic + 2*T_train + t + 1:T_plastic + 2*T_train + t + 2].T




    B = FIT.predict(X_test)
    #B = logistic.fit(X_train, y_train).predict(X_test)


    Seq[t + 1] = B[0]


#savetxt('activity.txt',x)
#savetxt('y_activity.txt',Y)






# OUTPUT


OutputSentence = []

for i in Seq:
    if i == 0:
        OutputSentence.append('a'),
    elif i == 1:
        OutputSentence.append('b'),
    elif i == 2:
        OutputSentence.append('c'),
    elif i == 3:
        OutputSentence.append('d'),
    elif i == 4:
        OutputSentence.append('e'),
    elif i == 5:
        OutputSentence.append('f'),
    elif i == 6:
        OutputSentence.append('g'),
    elif i == 7:
        OutputSentence.append('h'),
    elif i == 8:
        OutputSentence.append('i'),
    elif i == 9:
        OutputSentence.append('j'),
    elif i == 10:
        OutputSentence.append('k'),
    elif i == 11:
        OutputSentence.append('l'),
    elif i == 12:
        OutputSentence.append('m'),
    elif i == 13:
        OutputSentence.append('n'),
    elif i == 14:
        OutputSentence.append('o'),
    elif i == 15:
        OutputSentence.append('p'),
    elif i == 16:
        OutputSentence.append('q'),
    elif i == 17:
        OutputSentence.append('r'),
    elif i == 18:
        OutputSentence.append('s'),
    elif i == 19:
        OutputSentence.append('t'),
    elif i == 20:
        OutputSentence.append('u'),
    elif i == 21:
        OutputSentence.append('v'),
    elif i == 22:
        OutputSentence.append('w'),
    elif i == 23:
        OutputSentence.append('x'),
    elif i == 24:
        OutputSentence.append('y'),
    elif i == 25:
        OutputSentence.append('z'),
    elif i == 26:
        OutputSentence.append(' '),
    elif i == 27:
        OutputSentence.append('.'),

outputt = ''.join(OutputSentence)
